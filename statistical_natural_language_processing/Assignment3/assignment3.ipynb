{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\weights}{\\mathbf{w}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last assignment, you will apply deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding [[1]](http://anthology.aclweb.org/D/D13/D13-1020.pdf). Specifically, you will develop a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story [[2]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf). This sounds (and to an extent is) trivial for humans, however it is a quite difficult task for machines as it involves commonsense knowledge and temporal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "You are given a dataset of 45502 instances, each consisting of 5 sentences. Your system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:\n",
    "\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp.\n",
    "    Jan decided to get a new lamp.\n",
    "    Jan's lamp broke.\n",
    "\n",
    "your system needs to provide an answer in the following form:\n",
    "\n",
    "    2\t3\t4\t1\t0\n",
    "\n",
    "where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So \"`2`\" for \"`He went to the store.`\" means that this sentence should come 3rd in the correctly ordered target story. In This particular example, this order of indices corresponds to the following target story:\n",
    "\n",
    "    Jan's lamp broke.\n",
    "    Jan decided to get a new lamp.\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "To develop your model(s), we provide a training and a development datasets. The test dataset will be held out, and we will use it to evaluate your models. The test set is coming from the same task distribution, and you don't need to expect drastic changes in it.\n",
    "\n",
    "You will use [TensorFlow](https://www.tensorflow.org/) to build a deep learning model for the task. We provide a very crude system which solves the task with a low accuracy, and a set of additional functions you will have to use to save and load the model you create so that we can run it.\n",
    "\n",
    "As we have to run the notebooks of each submission, and as deep learning models take long time to train, your notebook **NEEDS** to conform to the following requirements:\n",
    "* You **NEED** to run your parameter optimisation offline, and provide your final model saved by using the provided function\n",
    "* The maximum size of a zip file you can upload to moodle is 160MB. We will **NOT** allow submissions larger than that.\n",
    "* We do not have time to train your models from scratch! You **NEED** to provide the full code you used for the training of your model, but by all means you **CANNOT** call the training method in the notebook you will send to us.\n",
    "* We will run these notebooks automatically. If your notebook runs the training procedure, in addition to loading the model, and we need to edit your code to stop the training, you will be penalised with **-20 points**.\n",
    "* If you do not provide a pretrained model, and rely on training your model on our machines, you will get **0 points**.\n",
    "* It needs to be tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get **0 points**.\n",
    "\n",
    "Running time and memory issues:\n",
    "* We have tested a possible solution on a mid-2014 MacBook Pro, and a few epochs of the model run in less than 3min. Thus it is possible to train a model on the data in reasonable time. However, be aware that you will need to run these models many times over, for a larger number of epochs (more elaborate models, trained on much larger datasets can train for weeks! However, this shouldn't be the case here.). If you find training times too long for your development cycle you can reduce the training set size. Once you have found a good solution you can increase the size again. Caveat: model parameters tuned on a smaller dataset may not be optimal for a larger training set.\n",
    "* In addition to this, as your submission is capped by size, feel free to experiment with different model sizes, numeric values of different precisions, filtering the vocabulary size, downscaling some vectors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "A non-exhaustive list of things you might want to give a try:\n",
    "- better tokenization\n",
    "- experiment with pre-trained word representations such as [word2vec](https://code.google.com/archive/p/word2vec/), or [GloVe](http://nlp.stanford.edu/projects/glove/). Be aware that these representations might take a lot of parameters in your model. Be sure you use only the words you expect in the training/dev set and account for OOV words. When saving the model parameters, pre-rained word embeddings can simply be used in the word embedding matrix of your model. As said, make sure that this word embedding matrix does not contain all of word2vec or GloVe. Your submission is limited, and we will not allow uploading nor using the whole representations set (up to 3GB!)\n",
    "- reduced sizes of word representations\n",
    "- bucketing and batching (our implementation is deliberately not a good one!)\n",
    "  - make sure to draw random batches from the data! (we do not provide this in our code!)\n",
    "- better models:\n",
    "  - stacked RNNs (see tf.nn.rnn_cell.MultiRNNCel\n",
    "  - bi-directional RNNs\n",
    "  - attention\n",
    "  - word-by-word attention\n",
    "  - conditional encoding\n",
    "  - get model inspirations from papers on nlp.stanford.edu/projects/snli/\n",
    "  - sequence-to-sequence encoder-decode architecture for producing the right ordering\n",
    "- better training procedure:\n",
    "  - different training algorithms\n",
    "  - dropout on the input and output embeddings (see tf.nn.dropout)\n",
    "  - L2 regularization (see tf.nn.l2_loss)\n",
    "  - gradient clipping (see tf.clip_by_value or tf.clip_by_norm)\n",
    "- model selection:\n",
    "  - early stopping\n",
    "- hyper-parameter optimization (e.g. random search or grid search (expensive!))\n",
    "    - initial learning rate\n",
    "    - dropout probability\n",
    "    - input and output size\n",
    "    - L2 regularization\n",
    "    - gradient clipping value\n",
    "    - batch size\n",
    "    - ...\n",
    "- post-processing\n",
    "  - for incorporating consistency constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to, and in `X` in `group_X` contains the number of your group.\n",
    "\n",
    "After you placed it there, **rename the notebook file** to `group_X`.\n",
    "\n",
    "The notebook is pre-set to save models in\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/model/\n",
    "\n",
    "Be sure not to tinker with that - we expect your submission to contain a `model` subdirectory with a single saved model! \n",
    "The saving procedure might overwrite the latest save, or not. Make sure you understand what it does, and upload only a single model! (for more details check tf.train.Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit, move nor copy these cells**.\n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit, move, nor copy these cells**.\n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "**If you edit, move or copy any of the setup, assessments and mark cells, you will be penalised with -20 points**.\n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment nor the dataset publicly, by uploading it online, emailing it to friends etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. Make sure you do not use any additional files other than your saved model.\n",
    "* Make sure that your solution runs linearly from start to end (no execution hops). We will run your notebook in that order.\n",
    "* **Before you submit, make sure your submission is tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get 0 points**.\n",
    "* **If running your notebook produces a trivially fixable error that we spot, we will correct it and penalise you with -20 points. Otherwise you will get 0 points for that solution.**\n",
    "* **Rename this notebook to your `group_X`** (where `X` is the number of your group), and adhere to the directory structure requirements, if you have not already done so. ** Failure to do so will result in -1 point.**\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Your submission should be a zip file containing the `group_X` directory, containing `group_X.ipynb` notebook, and the `model` directory with _____\n",
    "* Upload that file to the Moodle submission site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change, move or copy it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:56.249298",
     "start_time": "2016-12-20T12:04:54.376398"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. **Do not edit the next cell, nor copy/duplicate it**. Instead refer to the variables in your own code, and slice and dice them as you see fit (but do not change their values). \n",
    "For example, no one stops you from introducing, in the corresponding task section, `my_train` and `my_dev` variables that split the data into different folds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.110195",
     "start_time": "2016-12-20T12:04:56.251082"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Notice that the data is loaded from tab-separated files. The files are easy to read, and we provide the loading functions that load it into a simple data structure. Feel free to check details of the loading.\n",
    "\n",
    "The data structure at hand is an array of dictionaries, each containing a `story` and the `order` entry. `story` is a list of strings, and `order` is a list of integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.134033",
     "start_time": "2016-12-20T12:04:57.115270"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': [3, 2, 1, 0, 4],\n",
       " 'story': ['His parents understood and decided to make a change.',\n",
       "  'The doctors told his parents it was unhealthy.',\n",
       "  'Dan was overweight as well.',\n",
       "  \"Dan's parents were overweight.\",\n",
       "  'They got themselves and Dan on a diet.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Model implementation\n",
    "\n",
    "Your primary task in this assignment is to implement a model that produces the right order of the sentences in the dataset.\n",
    "\n",
    "### Preprocessing pipeline\n",
    "\n",
    "First, we construct a preprocessing pipeline, in our case `pipeline` function which takes care of:\n",
    "- out-of-vocabulary words\n",
    "- building a vocabulary (on the train set), and applying the same unaltered vocabulary on other sets (dev and test)\n",
    "- making sure that the length of input is the same for the train and dev/test sets (for fixed-sized models)\n",
    "\n",
    "You are free (and encouraged!) to do your own input processing function. Should you experiment with recurrent neural networks, you will find that you will need to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### OUR PIPELINE\n",
    "import re\n",
    "\n",
    "### REGEXP TOKENISER\n",
    "def tokenize(input):\n",
    "    token = re.compile('\\w+|[.?:,!()]')\n",
    "    return token.findall(input)\n",
    "    \n",
    "## PIPELINE IS MODIFIED TO ACCOUNT FOR TOKENISATION\n",
    "def pipeline(data, vocab=None, max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_sentences_len = []\n",
    "    data_orders = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        sents_len = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "            sents_len.append(len(sent))\n",
    "        data_sentences.append(sents)\n",
    "        data_sentences_len.append(sents_len)\n",
    "        data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len],\n",
    "                            vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_sentences_len = np.array(data_sentences_len, dtype=np.int32)\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "\n",
    "    return out_sentences, out_sentences_len, out_orders, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.842961",
     "start_time": "2016-12-20T12:04:57.136946"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert train set to integer IDs\n",
    "train_stories, train_stories_len, train_orders, vocab = pipeline(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that the `pipeline` function returns the necessary data for your computational graph feed - the required inputs in this case, as we will call this function to process your dev and test data. If you do not make sure that the same pipeline applied to the train set is applied to other datasets, your model may not work with that data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.925263",
     "start_time": "2016-12-20T12:04:59.844598"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# get the length of the longest sentence\n",
    "max_sent_len = train_stories.shape[2]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_stories_len, dev_orders, _ = pipeline(data_dev, vocab=vocab,\n",
    "                                                       max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### pre-trained word embeddings - new words loaded as OOV (see pipeline)\n",
    "def load_glove(path):\n",
    "    embeddings_index = {}\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def filter_embeddings(embeddings, vocab):\n",
    "    new_embeddings = {}\n",
    "    for word in vocab:\n",
    "        if word in embeddings:\n",
    "            new_embeddings[word] = embeddings[word]\n",
    "        elif word.lower() in embeddings:\n",
    "            new_embeddings[word.lower()] = embeddings[word.lower()]\n",
    "    return new_embeddings\n",
    "\n",
    "# embeddings_dict = load_glove('glove.6B/glove.6B.200d.txt')\n",
    "# embeddings_dict = filter_embeddings(embeddings_dict, vocab.keys())\n",
    "# np.save('embeddings_dict_200d.npy', embeddings_dict)\n",
    "\n",
    "# embeddings_dict = np.load('embeddings_dict_200d.npy').item()\n",
    "# print('embeddings loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the result of the `pipeline` with the `show_data_instance` function to make sure that your data loaded correctly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model we provide is a rudimentary, non-optimised model that essentially represents every word in a sentence with a fixed vector, sums these vectors up (per sentence) and puts a softmax at the end which aims to guess the order of sentences independently.\n",
    "\n",
    "First we define the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "LAYER NORMALISATION (IN FINAL MODEL)\n",
    "In our final model, we make use of the function LayerNormalizedLSTMCell. This function has been adapted from\n",
    "a r2t2.com implementation (see link below).   Layer normalisation is a feature\n",
    "recently published by Lei Ba et al. (2016). The function LayerNormalizedLSTMCell is essentially an edit of\n",
    "the built-in Tensorflow function 'tf.nn.rnn_cell.LSTMCell'. It accounts for layer normalisation applying\n",
    "the subfunction 'ln' to each gate output in a LSTM cell. The 'ln' function implements the layer normalisation,\n",
    "normalising to a variance of 1 and a mean of 0 a linear transformation output. Layer normalisation improves \n",
    "the performance of our model. More details on the following link:\n",
    "\n",
    "http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "'''\n",
    "def ln(tensor, scope = None, epsilon = 1e-5):\n",
    "    \"\"\" Layer normalizes a 2D tensor along its second axis \"\"\"\n",
    "    assert(len(tensor.get_shape()) == 2)\n",
    "    m, v = tf.nn.moments(tensor, [1], keep_dims=True)\n",
    "    if not isinstance(scope, str):\n",
    "        scope = ''\n",
    "    with tf.variable_scope(scope + 'layer_norm'):\n",
    "        scale = tf.get_variable('scale',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(1))\n",
    "        shift = tf.get_variable('shift',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(0))\n",
    "    LN_initial = (tensor - m) / tf.sqrt(v + epsilon)\n",
    "\n",
    "    return LN_initial * scale + shift\n",
    "\n",
    "class LayerNormalizedLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"\n",
    "    Adapted from TF's BasicLSTMCell to use Layer Normalization.\n",
    "    Note that state_is_tuple is always True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.nn.rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            c, h = state\n",
    "\n",
    "            # change bias argument to False since LN will add bias via shift\n",
    "            concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, False)\n",
    "\n",
    "            i, j, f, o = tf.split(1, 4, concat)\n",
    "\n",
    "            # add layer normalization to each gate\n",
    "            i = ln(i, scope = 'i/')\n",
    "            j = ln(j, scope = 'j/')\n",
    "            f = ln(f, scope = 'f/')\n",
    "            o = ln(o, scope = 'o/')\n",
    "\n",
    "            new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "                   self._activation(j))\n",
    "\n",
    "            # add layer_normalization in calculation of new hidden state\n",
    "            new_h = self._activation(ln(new_c, scope = 'new_h/')) * tf.nn.sigmoid(o)\n",
    "            new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "            return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.966529",
     "start_time": "2016-12-20T12:04:59.956638"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS ###\n",
    "target_size = 5\n",
    "vocab_size = len(vocab)\n",
    "n_rnn_layers = 2\n",
    "input_size = 200\n",
    "lstm_size = 50\n",
    "fc_size = 200\n",
    "output_size = 5\n",
    "dropout = 0.9\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Our final model makes use of truncated_normal technique for embedding initialisation. \n",
    "Random values are sampled from a normal distribution. \n",
    "Values with a magnitude greater than 2 standard deviations from the mean are not used and are resampled.\n",
    "'''\n",
    "def truncated_normal(shape, mean, std):\n",
    "    with tf.Session() as sess:\n",
    "        x = tf.Variable(tf.truncated_normal(shape, mean=mean, stddev=std,\n",
    "                                            dtype=np.float32))\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        return x.eval()\n",
    "\n",
    "### EMBEDDING CREATION\n",
    "def create_embeddings_W(embeddings_dict, vocab):\n",
    "    # compute statistics of pretrained embeddings\n",
    "    E = np.stack(list(embeddings_dict.values()))\n",
    "    mean = np.mean(E, axis=0)\n",
    "    std = np.std(E, axis=0)\n",
    "\n",
    "    rand_count = 0\n",
    "    vocab_size = len(vocab)\n",
    "    # embeddings_W = np.zeros((vocab_size, input_size), dtype=np.float32)\n",
    "    # embedding = np.random.uniform(-0.1, 0.1, input_size)\n",
    "    embeddings_W = truncated_normal([vocab_size, input_size], np.mean(mean), np.mean(std))\n",
    "    for word, index in vocab.items():\n",
    "        if word in embeddings_dict:\n",
    "            embedding = embeddings_dict[word]\n",
    "            embeddings_W[index,:] = embedding\n",
    "        elif word.lower() in embeddings_dict:\n",
    "            embedding = embeddings_dict[word.lower()]\n",
    "            embeddings_W[index,:] = embedding\n",
    "        else:\n",
    "            rand_count += 1\n",
    "\n",
    "    print('embedding W created winth %d rnad inits'%rand_count)\n",
    "    return embeddings_W\n",
    "\n",
    "# embeddings_W = create_embeddings_W(embeddings_dict, vocab)\n",
    "# np.save('embeddings_W.npy', embeddings_W)\n",
    "\n",
    "#embeddings_W = np.load('embeddings_W_200d.npy')\n",
    "#print('embeddings_W loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:00.995336",
     "start_time": "2016-12-20T12:04:59.968153"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### MODEL ###\n",
    "def linear_layer(inputs, num_outputs):\n",
    "    W = tf.truncated_normal([tf.shape(inputs)[1], num_outputs], stddev=0.1)\n",
    "    b = tf.constant(0.1, shape=[num_outputs])\n",
    "    return tf.matmul(inputs, W)\n",
    "\n",
    "## PLACEHOLDERS\n",
    "story = tf.placeholder(tf.int64, [None, None, None], \"story\")        # [batch_size x 5 x max_length]\n",
    "sentence_len = tf.placeholder(tf.int64, [None, None], \"sentence_len\") # [batch_size x 5]\n",
    "order = tf.placeholder(tf.int64, [None, None], \"order\")              # [batch_size x 5]\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "batch_size = tf.shape(story)[0]\n",
    "\n",
    "sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)]  # 5 times [batch_size x max_length]\n",
    "\n",
    "# Word embeddings\n",
    "# embeddings = tf.get_variable(\"W\", [vocab_size, input_size], trainable=True,\n",
    "#                              dtype=tf.float32)\n",
    "# embeddings = embeddings.assign(embeddings_W)\n",
    "embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, input_size], -1.0, 1.0), name=\"W\") \n",
    "\n",
    "\n",
    "sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # [batch_size x max_seq_length x input_size]\n",
    "                      for sentence in sentences]\n",
    "\n",
    "\n",
    "lstm = LayerNormalizedLSTMCell(lstm_size)\n",
    "lstm_dropout = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm_dropout] * n_rnn_layers, state_is_tuple=True)\n",
    "hs = []\n",
    "with tf.variable_scope(\"encoder\") as varscope:\n",
    "    for i in range(5):        \n",
    "        # START OF BIDIRECTIONAL - not in final model since it did not improve performance\n",
    "        #_,states  = tf.nn.bidirectional_dynamic_rnn(stacked_lstm,stacked_lstm, sentences_embedded[i], sequence_length=sentence_len[:,i], dtype=tf.float32)\n",
    "        #rnn_final_state_fw, rnn_final_state_bw = states\n",
    "        #hs.append(rnn_final_state_fw[-1].h)\n",
    "        #hs.append(rnn_final_state_bw[-1].h)\n",
    "        #varscope.reuse_variables()   \n",
    "        # END OF BIDIRECTIONAL \n",
    "        _, rnn_final_state = tf.nn.dynamic_rnn(stacked_lstm, sentences_embedded[i],\n",
    "                                               sequence_length=sentence_len[:,i],\n",
    "                                               dtype=tf.float32)\n",
    "        hs.append(rnn_final_state[-1].h)\n",
    "        varscope.reuse_variables()\n",
    "\n",
    "# START OF SEQ2SEQ ATTENTION ADD-ON  (not in final model)      \n",
    "# seq2seq\n",
    "# lstm = tf.nn.rnn_cell.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "# stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * n_rnn_layers,\n",
    "#                                           state_is_tuple=True)\n",
    "\n",
    "# encoder_inputs = hs\n",
    "# one_hot = tf.one_hot(order, 5, on_value=1.0, off_value=0.0, axis=-1, dtype=tf.float32)\n",
    "# pad with zeros for GO symbol\n",
    "# one_hot = tf.pad(one_hot, [[0,0],[1,0],[0,0]])\n",
    "# convert to list of inputs\n",
    "# decoder_inputs = [tf.reshape(x, [batch_size, 5]) for x in tf.split(1, 6, one_hot)]\n",
    "# num_decoder_symbols = 5\n",
    "# outputs, _ = attention_seq2seq(encoder_inputs, decoder_inputs, stacked_lstm,\n",
    "#                                num_decoder_symbols,num_heads=5,\n",
    "#                                feed_previous=feed_previous)\n",
    "\n",
    "# concat LSTM outputs and ignore the last one (EOS)\n",
    "# logits_flat = tf.concat(1, outputs[:-1])    # [batch_size x 5*input_size]        \n",
    "# END OF SEQ2SEQ ATTENTION ADD-ON        \n",
    "        \n",
    "# concat LSTM outputs\n",
    "h = tf.concat(1, hs)    # [batch_size x 5*input_size]\n",
    "h = tf.reshape(h, [batch_size, 5 * lstm_size])\n",
    "# FOR BIDIRECTIONAL\n",
    "# h = tf.reshape(h, [batch_size, 5 * lstm_size * 2 ])\n",
    "\n",
    "# FULLY CONNECTED LAYERS (in final model)\n",
    "# fc1 = tf.contrib.layers.fully_connected(h, fc_size)\n",
    "# fc1_drop = tf.nn.dropout(fc1, keep_prob)\n",
    "fc2 = tf.contrib.layers.fully_connected(h, fc_size)\n",
    "fc2_drop = tf.nn.dropout(fc2, keep_prob)\n",
    "fc3 = tf.contrib.layers.linear(fc2_drop, 5 * target_size)    # [batch_size x 5*target_size]\n",
    "logits = tf.reshape(fc3, [-1, 5, target_size])        # [batch_size x 5 x target_size]\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, order))\n",
    "# L2 regularisation attempt - not in final model\n",
    "# loss = (tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, order)) \n",
    "#                       + 0.01*tf.nn.l2_loss(h) + 0.01*tf.nn.l2_loss(fc2)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prediction function\n",
    "predict = tf.arg_max(logits, 2)\n",
    "correct_prediction = tf.equal(predict, order)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built our model, together with the loss and the prediction function, all we are left with now is to build an optimiser on the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:01.184409",
     "start_time": "2016-12-20T12:05:00.997016"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# ADAM AND NESTEROV MOMENTUM GIVE BEST PERFORMANCE\n",
    "#opt_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# EXPERIMENTING WITH GRADIENT CLIPPING to avoid exploding gradients (not in final model)\n",
    "#We attempted clipping gradients to specified min and max thresholds, \n",
    "# as well as clipping to a maximum L2-norm chosen in a given range\n",
    "#\n",
    "# gvs = optimizer.compute_gradients(loss)\n",
    "# capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
    "# capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
    "# opt_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "# Polynomial decaying learning rate:\n",
    "\n",
    "# gstep = tf.Variable(0, trainable=False)\n",
    "# start_lr = 0.01\n",
    "# end_lr = 0.001\n",
    "# decay_steps = 1000\n",
    "\n",
    "# learning_rate = tf.train.polynomial_decay(start_lr, gstep,\n",
    "#                                           decay_steps, end_lr,\n",
    "#                                             power=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training \n",
    "\n",
    "We defined the preprocessing pipeline, set the model up, so we can finally train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.615600",
     "start_time": "2016-12-20T12:05:01.186008"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# BATCH_SIZE = 1024\n",
    "# VAL_INTERVAL = 10\n",
    "# EXPERIMENT_NAME = 'stacked_lstm_50'\n",
    "\n",
    "# summary_loss = tf.placeholder(tf.float32)\n",
    "# summary_train_accuracy = tf.placeholder(tf.float32)\n",
    "# summary_dev_accuracy = tf.placeholder(tf.float32)\n",
    "# tf.scalar_summary(\"cross_entropy\", summary_loss)\n",
    "# tf.scalar_summary(\"train_accuracy\", summary_train_accuracy)\n",
    "# tf.scalar_summary(\"val_accuracy\", summary_dev_accuracy)\n",
    "# summary_op = tf.merge_all_summaries()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.initialize_all_variables())\n",
    "#     n = train_stories.shape[0]\n",
    "#     timestamp = time.strftime(\"%m.%d_%H.%M.%S\", time.gmtime())\n",
    "#     summary_writer = tf.train.SummaryWriter('tf_logs/' + EXPERIMENT_NAME +\n",
    "#                                             '-' + timestamp, sess.graph)\n",
    "#     iter_i = 0\n",
    "#     # 'early' stopping after 10-13 epochs to reach 'sweet spot' and not overtrain\n",
    "#     for epoch in range(12):\n",
    "#         t = time.time()\n",
    "#         print('----- Epoch', epoch, '-----')\n",
    "        \n",
    "#         # shuffle indices\n",
    "#         indices = np.arange(n)\n",
    "#         np.random.shuffle(indices)\n",
    "#         total_loss = 0\n",
    "#         total_acc = 0\n",
    "#         epoch_iter_i = 0\n",
    "#         for i in range(n // BATCH_SIZE):\n",
    "#             batch = indices[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "#             inst_story = train_stories[batch]\n",
    "#             inst_sentence_len = train_stories_len[batch]\n",
    "#             inst_order = train_orders[batch]\n",
    "#             # shuffle sentencs in stories\n",
    "#             for b in range(inst_story.shape[0]):\n",
    "#                 sent_idxs = np.arange(5)\n",
    "#                 np.random.shuffle(sent_idxs)\n",
    "#                 inst_story[b, :, :] = inst_story[b, sent_idxs, :]\n",
    "#                 inst_sentence_len[b, :] = inst_sentence_len[b, sent_idxs]\n",
    "#                 inst_order[b, :] = inst_order[b, sent_idxs]\n",
    "#             feed_dict = {story: inst_story, sentence_len: inst_sentence_len,\n",
    "#                          order: inst_order, keep_prob: dropout}\n",
    "#             _, current_acc, current_loss = sess.run([opt_op, accuracy, loss], feed_dict=feed_dict)\n",
    "#             total_loss += current_loss\n",
    "#             total_acc += current_acc\n",
    "#             epoch_iter_i += 1\n",
    "#             iter_i += 1\n",
    "#             if iter_i % VAL_INTERVAL == 0:\n",
    "#                 train_loss = total_loss / epoch_iter_i\n",
    "#                 train_accuracy = total_acc / epoch_iter_i\n",
    "#                 dev_feed_dict = {story: dev_stories, sentence_len: dev_stories_len,\n",
    "#                 order: dev_orders, keep_prob: 1.0}\n",
    "#                 dev_accuracy = sess.run(accuracy, feed_dict=dev_feed_dict)\n",
    "#                 print(' Iteration ', iter_i)\n",
    "#                 print(' Train loss: %.4f'%train_loss)\n",
    "#                 print(' Train accuracy: %.4f'%train_accuracy)\n",
    "#                 print(' Dev accuracy: %.4f'%dev_accuracy)\n",
    "#                 print('')\n",
    "#                 # save for tensorboard\n",
    "#                 summary_feed_dict = {summary_loss: train_loss,\n",
    "#                                      summary_train_accuracy: train_accuracy,\n",
    "#                                      summary_dev_accuracy: dev_accuracy}\n",
    "#                 summary = sess.run(summary_op, feed_dict=summary_feed_dict)\n",
    "#                 summary_writer.add_summary(summary, iter_i)\n",
    "        \n",
    "\n",
    "#         elapsed = time.time() - t\n",
    "#         print(' Time for epoch: %.2f'%elapsed)\n",
    "#         print('')\n",
    "#         print('')\n",
    "\n",
    "#     nn.save_model(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## error analysis\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     # LOAD THE MODEL\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.restore(sess, './model/model.checkpoint')\n",
    "#     dev_feed_dict = {\n",
    "#         story: dev_stories, sentence_len: dev_stories_len,\n",
    "#         order: dev_orders, keep_prob: 1.0}\n",
    "#     dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def show_data_instance(data_story, data_order, vocab):\n",
    "#     inverted_vocab = {value: key for key, value in vocab.items()}\n",
    "#     story_example = {}\n",
    "#     for i, elem in enumerate(data_story):\n",
    "#         x = list(inverted_vocab[ch] if ch in inverted_vocab else '<OOV>'\n",
    "#                  for ch in elem if ch != 0)\n",
    "#         story_example[data_order[i]] = \" \".join(x)\n",
    "#     print(' Order:\\n ', data_order)\n",
    "#     for (k, v) in sorted(story_example.items()):\n",
    "#         print(' ',v)\n",
    "\n",
    "# incorrect_idx = np.squeeze(np.argwhere(np.sum(dev_predicted == dev_orders, axis=1) > 0))\n",
    "# idx = 60\n",
    "# print('correct')\n",
    "# show_data_instance(dev_stories[incorrect_idx[idx]],\n",
    "#                    dev_orders[incorrect_idx[idx]], vocab)\n",
    "# print()\n",
    "# print('predicted')\n",
    "# show_data_instance(dev_stories[incorrect_idx[idx]],\n",
    "#                    dev_predicted[incorrect_idx[idx]], vocab)\n",
    "\n",
    "# print()\n",
    "# for i in range(6):\n",
    "#     n = np.sum(np.sum(dev_predicted == dev_orders, axis=1) == i)\n",
    "#     print('number of stories with %d mistakes: %d' % (i, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Assess Accuracy (50 pts) \n",
    "\n",
    "We assess how well your model performs on an unseen test set. We will look at the accuracy of the predicted sentence order, on sentence level, and will score them as followis:\n",
    "\n",
    "* 0 - 20 pts: 45% <= accuracy < 50%, linear\n",
    "* 20 - 40 pts: 50% <= accuracy < 55\n",
    "* 40 - 70 pts 55 <= accuracy < Best Result, linear\n",
    "\n",
    "The **linear** mapping maps any accuracy value between the lower and upper bound linearly to a score. For example, if your model's accuracy score is $acc=54.5\\%$, then your score is $20 + 20\\frac{acc-50}{55-50}$.\n",
    "\n",
    "The *Best-Result* accuracy is the maximum of the best accuracy the course organiser achieved, and the submitted accuracies scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Change the following lines so that they construct the test set in the same way you constructed the dev set in the code above. We will insert the test set instead of the dev set here. **`test_feed_dict` variable must stay named the same**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.755730",
     "start_time": "2016-12-20T12:05:54.617471"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data_test = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "# make sure you process this with the same pipeline as you processed your dev set\n",
    "test_stories, test_stories_len, test_orders, _ = pipeline(data_test, vocab=vocab,\n",
    "                                                          max_sent_len_=max_sent_len)\n",
    "\n",
    "# THIS VARIABLE MUST BE NAMED `test_feed_dict`\n",
    "test_feed_dict = {story: test_stories, sentence_len: test_stories_len,\n",
    "                  order: test_orders, keep_prob: 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:55.116609",
     "start_time": "2016-12-20T12:05:54.758571"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56782469267771241"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! ASSESSMENT 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "with tf.Session() as sess:\n",
    "    # LOAD THE MODEL\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    # RUN TEST SET EVALUATION\n",
    "    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "\n",
    "dev_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 1 is marked with ** __ points**. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "Enter a 750 words max description of your approach **in this cell**.\n",
    "Make sure to provide:\n",
    "- an **error analysis** of the types of errors your system makes\n",
    "- compare your system with the model we provide, focus on differences and draw useful comparations between them\n",
    "\n",
    "Should you need to include figures in your report, make sure they are Python-generated. For that, feel free to create new cells after this cell (before Assessment 2 cell). Link online images at your risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2</font>: Assess Description (30 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did, or you did nothing)\n",
    "* Creativity (10pts: we could not have come up with this, 0pts: Use only the provided model)\n",
    "* Substance (10pts: implemented complex state-of-the-art classifier, compared it to a simpler model, 0pts: Only use what is already there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "* **Tokenisation:** We used our own regural expression for tokenisation. We experimented with the NTLK tokeniser but this did not yield improved performance. \n",
    "\n",
    "* **Word representations:**  We achieved best performance using pretrained GloVe word embeddings (with 200 dimensions), compared to randomly initialized embeddings, and GloVe 50d, 100d. We only use embeddings for words in the training vocabulary with new words being handled as OOV. \n",
    "\n",
    "### Initialisation \n",
    "\n",
    "* **Bucketing and batching:** Bucketing yields no improvement in performance but improves training times considerably. Random batches are drawn. The batch size is set to 1024, being tuned as a hyper-parameter.\n",
    "\n",
    "* ** Truncated normal: ** We initialize the embedings weights with GloVe vectors. Words not contained in GloVe are initialized by truncated normal distribution as can be found in tensorflow.\n",
    "\n",
    "### The best model\n",
    "\n",
    " **Stacked RNN + MLP:** We feed sentences from the stories into a two layer stacked LSTM RNN with hidden layer of size 50. This produces fixed length embeddings for each of the sentences. We concatenate these embeddings into a single vector that is sent to two layer MLP (multilayer perceptron) layer the predicts the order of the stentences for each story. We used  200 units for the hidden layer.  Our final model includes a modified version of LSTM accounting for layer normalisation (see comments in code above or 'experimentation' section below).\n",
    "\n",
    "When compared to the provided model, our LSTM encoder replaces summation over the words in the sentences and our MLP replaces the single layer classifier in the original model.\n",
    "\n",
    "\n",
    "### Other models \n",
    "\n",
    "* **Sentence Ordering using RNN**  We implemented the state-of-the-art model (Logeswaran et al 2016) [4], however it did not performed bettern then our Stacked RNN + ML model. The code is provided below.\n",
    "\n",
    "* **Bi-directional RNN: ** Implemented but not included in the final model; it did not improve performance.\n",
    "\n",
    "* **Attention, conditional encoding, seq-2-seq: ** We modified the **seq2seq** module. Did not improve the performance of a stacked LSTM. We note the attention model works worse than simple seq2seq; this is due to a conceptual fault in the attention mechanism.  \n",
    "\n",
    "### Training procedure\n",
    "\n",
    "* **Optimiser choice:** Adam and Nesterov momentum optimisation gave best performance, with the latter requiring shorter timesteps and more time to converge. \n",
    "\n",
    "* **Gradient clipping:** There were no improvements in performance. We attempted clipping gradients to specified min and max thresholds, as well as clipping to a maximum L2-norm chosen in the range [0.5, 1, …, 10].\n",
    "\n",
    "* **Regularization (L2): ** Adding L2 regularisation has not improved performance. \n",
    "\n",
    "* We shuffled the sentences within the stories and also the order of the stories during training.\n",
    "\n",
    "* **Dropout:** For both MLP,  LSTM. The final model makes use of a global dropout of 0.9.\n",
    "\n",
    "\n",
    "### Model selection\n",
    "\n",
    "* **Early stopping: ** After monitoring the train/dev accuracy, we decide to stop the training procedure after 10-15 epochs (using the Adam optimiser). In this way overfitting is prevented. 10-15 epochs are sufficiently 'good' as a 'sweet spot' and also not too computationally expensive. \n",
    "\n",
    "### Hyperparameter optimisation\n",
    "\n",
    "* ** Tuning (batch size, LSTM layer size, learning rate, fc_size, dropout, etc): ** We attempted to tune the learning rate using grid search, starting from log-space, and then linear-space. The batch size is tuned in a range of power of 2s. \n",
    "\n",
    "* ** Learning rate: ** Using a decaying learning rate didn’t improve performance. We tried using polynomial decay, starting with a value which yielded promising results and using various decay steps and polynomial powers. These experiments did not improve performance. \n",
    "\n",
    "### Experimentation\n",
    "\n",
    "* ** Layer normalisation: ** \n",
    "In final model, we make use of the function LayerNormalizedLSTMCell. This function has been adapted from [1]. Layer normalisation is a feature recently published by Lei Ba et al. (2016) [2]. \n",
    "\n",
    "### Error Analysis\n",
    "During model training, we monitored our train/test errors using Tensorboard.\n",
    "After predictions, we observed frequencies of the number of mistakes made in the ordering of each of the 5 sentences for all stories. On the dev data, these amounted to 35, 219, 511, 579, 301, 226 story instances where we made 0,1,…5 mistakes, respectively. \n",
    "Additionally, we found that some errors were due to assigning the same order index to two sentences in a story, such as [3,1,0,1,4]. \n",
    "Other errors were understandable in cases where the misclassification was between two sentences which could easily be interchanged without affecting the meaning behind the story. An example of this is story index 300 from the dev set.\n",
    "\n",
    "### References \n",
    "[1] http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "[2] Layer normalisation (Lei Ba et al 2016) https://arxiv.org/abs/1607.06450\n",
    "[3] Sentence Ordering using RNN (Logeswaran et al 2016)\n",
    "[4] Recurrent Model for Visual attention (Mnih et al 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ========================== CODE OF OTHER THINGS WE'VE TRIED =====================================================\n",
    "\n",
    "\n",
    "# ==================== Sentence Ordering using Recurrent Neural Networks ==========================================\n",
    "# ============================== https://arxiv.org/abs/1611.02654 =================================================\n",
    "\n",
    "# def attention_encoder(initial_state, attention_states, cell,\n",
    "#                       feed_previous=False):\n",
    "#     with tf.variable_scope(\"attention_encoder\"):\n",
    "#         output_size = cell.output_size\n",
    "\n",
    "#         batch_size = tf.shape(attention_states)[0]\n",
    "#         attn_length = attention_states.get_shape()[1].value\n",
    "#         attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "#         # attention_states should be batch_size x 5 x attn_size\n",
    "\n",
    "#         # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "#         hidden = tf.reshape(\n",
    "#             attention_states, [-1, attn_length, 1, attn_size])\n",
    "\n",
    "#         hidden_size = 500\n",
    "\n",
    "#         W_fc1 = tf.get_variable(\"W_fc1a\", [1, 1, attn_size + output_size, hidden_size])\n",
    "#         b_fc1 = tf.get_variable(\"b_fc1a\", [hidden_size])\n",
    "#         W_fc2 = tf.get_variable(\"W_fc2a\", [1, 1, hidden_size, 1])\n",
    "#         b_fc2 = tf.get_variable(\"b_fc2a\", [1])\n",
    "\n",
    "#         state = initial_state\n",
    "\n",
    "#         def attention_mask(h):\n",
    "#             h_size = h.get_shape()[1].value\n",
    "#             h_reshape = tf.reshape(h, [-1, 1, 1, h_size])\n",
    "#             h_tile = tf.tile(h_reshape, [1, attn_length, 1, 1])\n",
    "\n",
    "#             x = tf.concat(3, [hidden, h_tile])\n",
    "\n",
    "#             layer1 = tf.tanh(tf.nn.conv2d(x, W_fc1, [1, 1, 1, 1], \"SAME\") + b_fc1)\n",
    "#             layer2 = tf.nn.conv2d(layer1, W_fc2, [1, 1, 1, 1], \"SAME\") + b_fc2\n",
    "\n",
    "#             e = tf.reshape(layer2, [-1, attn_length])\n",
    "#             a = tf.nn.softmax(e)\n",
    "\n",
    "#             return a\n",
    "\n",
    "#         def attention(a):\n",
    "#             a = tf.reshape(a, [-1, attn_length, 1, 1])\n",
    "#             # Now calculate the attention-weighted vector d.\n",
    "#             s_att = tf.reduce_sum(a * hidden, [1, 2])\n",
    "#             s_att = tf.reshape(s_att, [-1, attn_size])\n",
    "#             return s_att\n",
    "\n",
    "#         zeros = tf.zeros([batch_size, 1], dtype=tf.float32)\n",
    "#         batch_attn_size = tf.pack([batch_size, attn_size])\n",
    "#         s_att = tf.zeros(batch_attn_size, dtype=tf.float32)\n",
    "#         # Ensure the second shape of attention vectors is set.\n",
    "#         s_att.set_shape([None, attn_size])\n",
    "#         for i in range(10):\n",
    "#             if i > 0:\n",
    "#                 tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "#             # Run the RNN.\n",
    "#             _, state = cell(zeros, state)\n",
    "\n",
    "#             # Get h state\n",
    "#             h = state.h\n",
    "\n",
    "#             # Run the attention mechanism.\n",
    "#             a_predict = attention_mask(h)\n",
    "\n",
    "#             # apply attention_mask on sentences\n",
    "#             s_att = attention(a_predict)\n",
    "\n",
    "#             h_size = h.get_shape()[1].value\n",
    "\n",
    "#             h = tf.nn.seq2seq.linear([h, s_att], h_size, True)\n",
    "\n",
    "#             state = tf.nn.rnn_cell.LSTMStateTuple(state.c, h)\n",
    "\n",
    "#         return state\n",
    "\n",
    "\n",
    "# def attention_decoder(decoder_inputs, initial_state, attention_states, cell,\n",
    "#                       feed_previous=False):\n",
    "#     with tf.variable_scope(\"attention_decoder\"):\n",
    "#         output_size = cell.output_size\n",
    "\n",
    "#         batch_size = tf.shape(decoder_inputs[0])[0]\n",
    "#         attn_length = attention_states.get_shape()[1].value\n",
    "#         attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "#         # attention_states should be batch_size x 5 x attn_size\n",
    "\n",
    "#         hidden = tf.reshape(attention_states, [-1, attn_length, 1, attn_size])\n",
    "\n",
    "#         hidden_size = 500\n",
    "\n",
    "#         W_fc1 = tf.get_variable(\"W_fc1\", [1, 1, attn_size + output_size, hidden_size])\n",
    "#         b_fc1 = tf.get_variable(\"b_fc1\", [hidden_size])\n",
    "#         W_fc2 = tf.get_variable(\"W_fc2\", [1, 1, hidden_size, 1])\n",
    "#         b_fc2 = tf.get_variable(\"b_fc2\", [1])\n",
    "\n",
    "#         state = initial_state\n",
    "\n",
    "#         def attention_mask(h):\n",
    "#             h_size = h.get_shape()[1].value\n",
    "#             h_reshape = tf.reshape(h, [-1, 1, 1, h_size])\n",
    "#             h_tile = tf.tile(h_reshape, [1, attn_length, 1, 1])\n",
    "\n",
    "#             x = tf.concat(3, [hidden, h_tile])\n",
    "\n",
    "#             layer1 = tf.tanh(tf.nn.conv2d(x, W_fc1, [1, 1, 1, 1], \"SAME\") + b_fc1)\n",
    "#             layer2 = tf.nn.conv2d(layer1, W_fc2, [1, 1, 1, 1], \"SAME\") + b_fc2\n",
    "\n",
    "#             e = tf.reshape(layer2, [-1, attn_length])\n",
    "#             a = tf.nn.softmax(e)\n",
    "\n",
    "#             return a\n",
    "\n",
    "#         def attention(a):\n",
    "#             a = tf.reshape(a, [-1, attn_length, 1, 1])\n",
    "#             # Now calculate the attention-weighted vector d.\n",
    "#             s_att = tf.reduce_sum(a * hidden, [1, 2])\n",
    "#             s_att = tf.reshape(s_att, [-1, attn_size])\n",
    "#             return s_att\n",
    "\n",
    "\n",
    "#         a_predicts = []\n",
    "#         reuse = None\n",
    "#         batch_attn_size = tf.pack([batch_size, attn_size])\n",
    "#         s_att = tf.zeros(batch_attn_size, dtype=tf.float32)\n",
    "#         # Ensure the second shape of attention vectors is set.\n",
    "#         s_att.set_shape([None, attn_size])\n",
    "#         for i, a in enumerate(decoder_inputs):\n",
    "#             if i > 0:\n",
    "#                 tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "#             # If feed_previous is set, we use the previous predicted mask.\n",
    "#             if feed_previous and i > 0:\n",
    "#                 a = a_predict\n",
    "\n",
    "#             # apply attention_mask on sentences\n",
    "#             s_att = attention(a)\n",
    "\n",
    "#             # Run the RNN.\n",
    "#             _, state = cell(s_att, state)\n",
    "\n",
    "#             # Get h state of the last LSTM layer\n",
    "#             h = state.h\n",
    "\n",
    "#             # Run the attention mechanism.\n",
    "#             a_predict = attention_mask(h)\n",
    "\n",
    "#             a_predicts.append(a_predict)\n",
    "\n",
    "#         return a_predicts\n",
    "\n",
    "\n",
    "\n",
    "# def attention_seq2seq(encoder_inputs, decoder_inputs, cell,\n",
    "#                       feed_previous=False):\n",
    "#     with tf.variable_scope(\"rnn_seq2seq\"):\n",
    "#         # First calculate a concatenation of encoder outputs to put attention on.\n",
    "#         attn_size = encoder_inputs[0].get_shape()[1].value\n",
    "#         top_states = [tf.reshape(e, [-1, 1, attn_size])\n",
    "#                       for e in encoder_inputs]\n",
    "#         attention_states = tf.concat(1, top_states)\n",
    "\n",
    "#         batch_size = tf.shape(encoder_inputs[0])[0]\n",
    "\n",
    "#         encoder_init_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "#         # # Encoder.\n",
    "#         encoder_state = attention_encoder(\n",
    "#             encoder_init_state, attention_states, cell)\n",
    "\n",
    "#         # Decoder.\n",
    "#         #  we construct 2 graphs and use cond.\n",
    "#         def decoder(feed_previous_bool):\n",
    "#             reuse = None if feed_previous_bool else True\n",
    "#             with tf.variable_scope(tf.get_variable_scope(), reuse=reuse):\n",
    "#                 outputs = attention_decoder(\n",
    "#                     decoder_inputs, encoder_state, attention_states, cell,\n",
    "#                     feed_previous=feed_previous_bool)\n",
    "#                 return outputs\n",
    "\n",
    "#         outputs = tf.cond(feed_previous,\n",
    "#                           lambda: decoder(True),\n",
    "#                           lambda: decoder(False))\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LAYER NORMALISATION (IN FINAL MODEL)\n",
    "# In our final model, we make use of the function LayerNormalizedLSTMCell. This function has been adapted from\n",
    "# a r2t2.com implementation (see link below).   Layer normalisation is a feature\n",
    "# recently published by Lei Ba et al. (2016). The function LayerNormalizedLSTMCell is essentially an edit of\n",
    "# the built-in Tensorflow function 'tf.nn.rnn_cell.LSTMCell'. It accounts for layer normalisation applying\n",
    "# the subfunction 'ln' to each gate output in a LSTM cell. The 'ln' function implements the layer normalisation,\n",
    "# normalising to a variance of 1 and a mean of 0 a linear transformation output. Layer normalisation improves \n",
    "# the performance of our model. More details on the following link:\n",
    "\n",
    "# http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ATTENTION SEQUENCE TO SEQUENCE IMPLEMENTATION\n",
    "# # (based on tf.nn.seq2seq)\n",
    "\n",
    "# # (not in final model - did not improve performance)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "# from six.moves import zip     # pylint: disable=redefined-builtin\n",
    "\n",
    "# from tensorflow.python.framework import dtypes\n",
    "# from tensorflow.python.framework import ops\n",
    "# from tensorflow.python.ops import array_ops\n",
    "# from tensorflow.python.ops import control_flow_ops\n",
    "# from tensorflow.python.ops import embedding_ops\n",
    "# from tensorflow.python.ops import math_ops\n",
    "# from tensorflow.python.ops import nn_ops\n",
    "# from tensorflow.python.ops import rnn\n",
    "# from tensorflow.python.ops import rnn_cell\n",
    "# from tensorflow.python.ops import variable_scope\n",
    "# from tensorflow.python.util import nest\n",
    "\n",
    "# import seq2seq\n",
    "\n",
    "# def my_loop_function(prev, _):\n",
    "#     prev_symbol = math_ops.argmax(prev, 1)\n",
    "#     one_hot = tf.one_hot(prev_symbol, 5, on_value=1.0, off_value=0.0, axis=-1,\n",
    "#                          dtype=tf.float32)\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "# def attention_decoder(decoder_inputs, initial_state, attention_states, cell,\n",
    "#                       output_size=None, num_heads=1, loop_function=None,\n",
    "#                       dtype=dtypes.float32, scope=None,\n",
    "#                       initial_state_attention=False):\n",
    "#     if not decoder_inputs:\n",
    "#         raise ValueError(\"Must provide at least 1 input to attention decoder.\")\n",
    "#     if num_heads < 1:\n",
    "#         raise ValueError(\"With less than 1 heads, use a non-attention decoder.\")\n",
    "#     if not attention_states.get_shape()[1:2].is_fully_defined():\n",
    "#         raise ValueError(\"Shape[1] and [2] of attention_states must be known: %s\"\n",
    "#                         % attention_states.get_shape())\n",
    "#     if output_size is None:\n",
    "#         output_size = cell.output_size\n",
    "\n",
    "#     with variable_scope.variable_scope(scope or \"attention_decoder\"):\n",
    "#         batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n",
    "#         attn_length = attention_states.get_shape()[1].value\n",
    "#         attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "#         # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "#         hidden = array_ops.reshape(\n",
    "#             attention_states, [-1, attn_length, 1, attn_size])\n",
    "#         hidden_features = []\n",
    "#         v = []\n",
    "#         attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "#         for a in xrange(num_heads):\n",
    "#             k = variable_scope.get_variable(\"AttnW_%d\" % a,\n",
    "#                                             [1, 1, attn_size, attention_vec_size])\n",
    "#             hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "#             v.append(variable_scope.get_variable(\"AttnV_%d\" % a,\n",
    "#                                                  [attention_vec_size]))\n",
    "\n",
    "#         state = initial_state\n",
    "\n",
    "#         def attention(query):\n",
    "#             \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n",
    "#             ds = []  # Results of attention reads will be stored here.\n",
    "#             if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n",
    "#                 query_list = nest.flatten(query)\n",
    "#                 for q in query_list:  # Check that ndims == 2 if specified.\n",
    "#                     ndims = q.get_shape().ndims\n",
    "#                     if ndims:\n",
    "#                         assert ndims == 2\n",
    "#                 query = array_ops.concat(1, query_list)\n",
    "#             for a in xrange(num_heads):\n",
    "#                 with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "#                     y = tf.nn.seq2seq.linear(query, attention_vec_size, True)\n",
    "#                     y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "#                     # Attention mask is a softmax of v^T * tanh(...).\n",
    "#                     s = math_ops.reduce_sum(\n",
    "#                         v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "#                     a = nn_ops.softmax(s)\n",
    "#                     # Now calculate the attention-weighted vector d.\n",
    "#                     d = math_ops.reduce_sum(\n",
    "#                         array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n",
    "#                         [1, 2])\n",
    "#                     ds.append(array_ops.reshape(d, [-1, attn_size]))\n",
    "#             return ds\n",
    "\n",
    "#         outputs = []\n",
    "#         prev = None\n",
    "#         batch_attn_size = array_ops.pack([batch_size, attn_size])\n",
    "#         attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n",
    "#                  for _ in xrange(num_heads)]\n",
    "#         for a in attns:  # Ensure the second shape of attention vectors is set.\n",
    "#             a.set_shape([None, attn_size])\n",
    "#         if initial_state_attention:\n",
    "#             attns = attention(initial_state)\n",
    "#         for i, inp in enumerate(decoder_inputs):\n",
    "#             if i > 0:\n",
    "#                 variable_scope.get_variable_scope().reuse_variables()\n",
    "#             # If loop_function is set, we use it instead of decoder_inputs.\n",
    "#             if loop_function is not None and prev is not None:\n",
    "#                 with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "#                     inp = loop_function(prev, i)\n",
    "#             # Merge input and previous attentions into one vector of the right size.\n",
    "#             input_size = inp.get_shape().with_rank(2)[1]\n",
    "#             if input_size.value is None:\n",
    "#                 raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "#             x = tf.nn.seq2seq.linear([inp] + attns, input_size, True)\n",
    "#             # Run the RNN.\n",
    "#             cell_output, state = cell(x, state)\n",
    "#             # Run the attention mechanism.\n",
    "#             if i == 0 and initial_state_attention:\n",
    "#                 with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "#                                                    reuse=True):\n",
    "#                     attns = attention(state)\n",
    "#             else:\n",
    "#                 attns = attention(state)\n",
    "\n",
    "#             with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "#                 output = tf.nn.seq2seq.linear([cell_output] + attns, output_size, True)\n",
    "#             if loop_function is not None:\n",
    "#                 prev = output\n",
    "#             outputs.append(output)\n",
    "\n",
    "#     return outputs, state\n",
    "\n",
    "\n",
    "\n",
    "# def my_attention_decoder( decoder_inputs, initial_state, attention_states,\n",
    "#                           cell, num_heads=1, output_size=None,\n",
    "#                           feed_previous=False, dtype=dtypes.float32, scope=None,\n",
    "#                           initial_state_attention=False):\n",
    "#     if output_size is None:\n",
    "#         output_size = cell.output_size\n",
    "#     with variable_scope.variable_scope(scope or \"my_attention_decoder\"):\n",
    "#         loop_function = my_loop_function if feed_previous else None\n",
    "#         return seq2seq.attention_decoder(\n",
    "#             decoder_inputs, initial_state, attention_states, cell,\n",
    "#             output_size=output_size, num_heads=num_heads,\n",
    "#             loop_function=loop_function,\n",
    "#             initial_state_attention=initial_state_attention)\n",
    "#         # return seq2seq.rnn_decoder(\n",
    "#         #     decoder_inputs, initial_state, cell, loop_function=loop_function)\n",
    "\n",
    "\n",
    "\n",
    "# def attention_seq2seq(encoder_inputs, decoder_inputs, cell, num_decoder_symbols,\n",
    "#                       num_heads=1, feed_previous=False, dtype=dtypes.float32,\n",
    "#                       scope=None, initial_state_attention=False):\n",
    "#     with variable_scope.variable_scope(scope or \"rnn_seq2seq\"):\n",
    "#         # Encoder.\n",
    "#         encoder_outputs, encoder_state = rnn.rnn(\n",
    "#             cell, encoder_inputs, dtype=dtype)\n",
    "#         # First calculate a concatenation of encoder outputs to put attention on.\n",
    "#         top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n",
    "#                       for e in encoder_outputs]\n",
    "#         attention_states = array_ops.concat(1, top_states)\n",
    "\n",
    "#         # Decoder.\n",
    "#         cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "#         output_size = num_decoder_symbols\n",
    "#         #  we construct 2 graphs and use cond.\n",
    "#         def decoder(feed_previous_bool):\n",
    "#             reuse = None if feed_previous_bool else True\n",
    "#             with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "#                                                reuse=reuse):\n",
    "#                 outputs, state = my_attention_decoder(\n",
    "#                     decoder_inputs, encoder_state, attention_states, cell,\n",
    "#                     num_heads=num_heads, output_size=output_size,\n",
    "#                     feed_previous=feed_previous_bool,\n",
    "#                     initial_state_attention=initial_state_attention)\n",
    "#                 state_list = [state]\n",
    "#                 if nest.is_sequence(state):\n",
    "#                     state_list = nest.flatten(state)\n",
    "#                 return outputs + state_list\n",
    "\n",
    "\n",
    "#         outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "#                                                   lambda: decoder(True),\n",
    "#                                                   lambda: decoder(False))\n",
    "#         outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "#         state_list = outputs_and_state[outputs_len:]\n",
    "#         state = state_list[0]\n",
    "#         if nest.is_sequence(encoder_state):\n",
    "#             state = nest.pack_sequence_as(structure=encoder_state,\n",
    "#                                           flat_sequence=state_list)\n",
    "#         return outputs_and_state[:outputs_len], state\n",
    "\n",
    "\n",
    "\n",
    "# START OF BIDIRECTIONAL - not in final model since it did not improve performance\n",
    "#_,states  = tf.nn.bidirectional_dynamic_rnn(stacked_lstm,stacked_lstm, sentences_embedded[i], sequence_length=sentence_len[:,i], dtype=tf.float32)\n",
    "#rnn_final_state_fw, rnn_final_state_bw = states\n",
    "#hs.append(rnn_final_state_fw[-1].h)\n",
    "#hs.append(rnn_final_state_bw[-1].h)\n",
    "#varscope.reuse_variables()   \n",
    "# END OF BIDIRECTIONAL \n",
    "        \n",
    "\n",
    "# START OF SEQ2SEQ ATTENTION ADD-ON  (not in final model)      \n",
    "# seq2seq\n",
    "# lstm = tf.nn.rnn_cell.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "# stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * n_rnn_layers,\n",
    "#                                           state_is_tuple=True)\n",
    "\n",
    "# encoder_inputs = hs\n",
    "# one_hot = tf.one_hot(order, 5, on_value=1.0, off_value=0.0, axis=-1, dtype=tf.float32)\n",
    "# pad with zeros for GO symbol\n",
    "# TODO: make GO symbol learnable?\n",
    "# one_hot = tf.pad(one_hot, [[0,0],[1,0],[0,0]])\n",
    "# convert to list of inputs\n",
    "# decoder_inputs = [tf.reshape(x, [batch_size, 5]) for x in tf.split(1, 6, one_hot)]\n",
    "# num_decoder_symbols = 5\n",
    "# outputs, _ = attention_seq2seq(encoder_inputs, decoder_inputs, stacked_lstm,\n",
    "#                                num_decoder_symbols,num_heads=5,\n",
    "#                                feed_previous=feed_previous)\n",
    "\n",
    "# concat LSTM outputs and ignore the last one (EOS)\n",
    "# logits_flat = tf.concat(1, outputs[:-1])    # [batch_size x 5*input_size]        \n",
    "# END OF SEQ2SEQ ATTENTION ADD-ON        \n",
    "        \n",
    "# concat LSTM outputs\n",
    "#h = tf.concat(1, hs)    # [batch_size x 5*input_size]\n",
    "# FOR BIDIRECTIONAL\n",
    "# h = tf.reshape(h, [batch_size, 5 * lstm_size * 2 ])\n",
    "\n",
    "# ADAM AND NESTEROV MOMENTUM GIVE BEST PERFORMANCE\n",
    "# opt_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# EXPERIMENTING WITH GRADIENT CLIPPING to avoid exploding gradients (not in final model)\n",
    "#We attempted clipping gradients to specified min and max thresholds, \n",
    "# as well as clipping to a maximum L2-norm chosen in a given range\n",
    "#\n",
    "# gvs = optimizer.compute_gradients(loss)\n",
    "# capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
    "# capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
    "# opt_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 2 is marked with ** __ points**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Final mark</font>: Your solution to Assignment 3 is marked with ** __points**. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
